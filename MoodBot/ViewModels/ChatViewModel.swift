//
//  ChatViewModel.swift
//  MoodBot
//
//  Created by Yen Hung Cheng on 2023/6/30.
//

import SwiftUI
import OpenAISwift
import AVFoundation
import NaturalLanguage

final class ViewModel: ObservableObject {
    init() {}
    
    private var client: OpenAISwift?
    
    func setup() {
        client = OpenAISwift(authToken: "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")
        
    }
    
    func send(text: String, maxTokens: Int,
              completion: @escaping (String) -> Void) {
        client?.sendCompletion(with: text,
                               maxTokens: maxTokens,
                               completionHandler: { result in
            switch result {
            case .success(let model):
                let output = model.choices?.first?.text ?? ""
                completion(output)
            case .failure:
                break
            }
        })
    }
}


struct ChatViewModel: View {
    
    @ObservedObject var viewModel = ViewModel()
    @State var text = ""
    @State var models = [String]()
    
    // å›žè¦†çš„èªžè¨€ï¼Œé è¨­ç‚º è‹±æ–‡
    @State private var replyLanguage = "English"
    @State var selectedSegment1: Segment = .English
    @State var token = 200
    @State private var showBool = false

    @StateObject private var speechManager = SpeechManager()
    
    // AR
    @State private var analysis = "analysis"
    @State private var isDetecting = false
    @State var showAR: Bool = false
    @State var mood: String = ""

    

    
    
    @State var openAI = [
        "CNN stands for Convolutional Neural Network. It is a type of deep learning algorithm that is widely used for image classification, object detection, and computer vision tasks. CNNs are particularly effective in analyzing visual data due to their ability to automatically learn and extract relevant features from images.The key components of a CNN are convolutional layers, pooling layers, and fully connected layers. Convolutional layers apply convolution operations to input images, applying filters to detect different features. Pooling layers downsample the feature maps, reducing their spatial dimensions. Fully connected layers connect the extracted features to the output layer for classification or regression.",
        "The Transformer architecture is based on the concept of self-attention mechanism, also known as scaled dot-product attention. It enables the model to focus on different parts of the input sequence while processing it. Unlike recurrent neural networks (RNNs) or convolutional neural networks (CNNs), the Transformer does not rely on sequential or convolutional operations, making it highly parallelizable and more efficient for processing long-range dependencies.",
        "ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
        
    ]
    
    @State var moodArray = [String]()
    
    
    var body: some View {
        NavigationStack {
            ZStack {
                VStack(alignment: .leading, spacing: 20) {
                    ScrollView {
                        VStack(alignment: .leading, spacing: 20) {
                            ForEach(models.indices, id: \.self) { index in
                                if (index % 2) != 0 {
                                    TextRowView(text: models[index], image: Image("openAI"), bool: true, doneBool: $showAR)
                                }else {
                                    TextRowView(text: models[index], image: Image(""), bool: false, doneBool: $showAR)
                                }
                            }
                            Spacer()
                        }
                    }
                    
                    // åˆ¤æ–·æ˜¯å¦æœ‰åœ¨ä½¿ç”¨ AVSpeechSynthesizer èªªè©±
                    if speechManager.isSpeaking {
                        HStack(spacing: 50) {
                            Spacer()
                            Text("Speaking...")
                            Spacer()
                            Button {
                                speechManager.synthesizer.stopSpeaking(at: .immediate)
                            } label: {
                                Image(systemName: "stop.circle")
                                    .foregroundColor(.red)
                            }
                        }
                    }else {
                        HStack {
                            TextField("Message", text: $text, onCommit: {
                                send()
                            })
                            .textFieldStyle(.roundedBorder)
                            
                            if text == "" {
                                Image(systemName: "arrow.up.circle.fill")
                                    .foregroundColor(.gray)
                            }else {
                                Button {
                                    send()
                                    print(models)
                                } label: {
                                    Image(systemName: "arrow.up.circle.fill")
                                        .foregroundColor(Color(red: 102/255, green: 66/255, blue: 1))
                                }
                            }
                        }
                    }
                    
                    
                    
                }
                .onAppear {
                    viewModel.setup()
                }
                .toolbar {
                    Button {
                        showBool.toggle()
                    } label: {
                        Image(systemName: "gearshape")
                    }
                    .sheet(isPresented: $showBool) {
                        LanguageViewModel(selectedSegment1: $selectedSegment1, language: $replyLanguage, closeBool: $showBool, token: $token)
                            .presentationDetents([.height(250), .medium])
                    }
                }
                .padding()
                // ç¬¬äºŒå±¤
                if showAR {
                    VStack {
                        Text(analysis)
                            .font(.title2)
                            .padding()
                            .background(.black)
                            .foregroundColor(.green)
                            .cornerRadius(25)
                        Spacer()
                        ARViewContainer(analysis: $analysis, isDetecting: $showAR)
                            .frame(width: 1, height: 1) // è¨­ç½® ARView çš„é«˜åº¦
                            .onAppear {
                                DispatchQueue.main.asyncAfter(deadline: .now() + 5) {
                                    showAR = false
//                                    switch analysis {
//                                    case "ä¸æ˜¯å¾ˆæ»¿æ„ðŸ™‚":
//                                        mood = "ðŸ™‚"
//                                    case "éžå¸¸ä¸æ»¿æ„ðŸ˜¡":
//                                        mood = "ðŸ˜¡"
//                                    default:
//                                        mood = "ðŸ˜„"
//                                    }
                                    
                                }
                                
                            }
                    }
                }
                
                
            }
        }
        
        
    }
    
    func send() {
        guard !text.trimmingCharacters(in: .whitespaces).isEmpty else {
            return
        }

        models.append("\(text)")

        viewModel.send(text: text, maxTokens: token) { response in
            DispatchQueue.main.async {
//                self.models.append(response.trimmingCharacters(in: .whitespacesAndNewlines))
                let message = openAI[0]
                models.append(openAI.remove(at: 0))
//                print(response.trimmingCharacters(in: .whitespacesAndNewlines))
                speechManager.speak(text: message)
                // é‡ç½®æ–‡æœ¬
                self.text = ""
                print(openAI)
            }
        }
    }
    
//    func send() {
//        guard !text.trimmingCharacters(in: .whitespaces).isEmpty else {
//            return
//        }
//
//        models.append("\(text)")
//
////        let chatMessage = "?           Hello! How can I assist you today?"
////        let chatMessage = "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
//        let chatMessage = "ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
//
//
//
//        viewModel.send(text: chatMessage, maxTokens: token) { response in
//            DispatchQueue.main.async {
//
//                // newresponse ä¿å­˜æœ€åˆçš„ response
////                var newresponse = response
//                var newresponse = chatMessage
//
//                // æœ‰æ™‚å€™æœ€å‰é¢æœƒç”¢ç”Ÿ ? å¿…é ˆç§»é™¤
//                // response å®šç¾©æ˜¯ let ä¸èƒ½ç§»é™¤
//                if newresponse.hasPrefix("?") || newresponse.hasPrefix("!") {
//                    newresponse.removeFirst()
//                }
////                self.models.append(newresponse.trimmingCharacters(in: .whitespacesAndNewlines))
//                // å°‡ç©ºç™½éƒ¨åˆ†ç§»é™¤
//                models.append(newresponse.trimmingCharacters(in: .whitespacesAndNewlines))
//
//                print(newresponse.trimmingCharacters(in: .whitespacesAndNewlines))
//                speechManager.speak(text: newresponse.trimmingCharacters(in: .whitespacesAndNewlines))
//
//                // é‡ç½®æ–‡æœ¬
//                self.text = ""
//            }
//        }
//    }

    
    
    
}

struct ChatViewModel_Previews: PreviewProvider {
    static var previews: some View {
        ChatViewModel()
    }
}


class SpeechManager: NSObject, ObservableObject {
    
    @Published var isSpeaking = false
    let synthesizer = AVSpeechSynthesizer()
    func speak(text: String) {
        let utterance = AVSpeechUtterance(string: text)
        utterance.voice = AVSpeechSynthesisVoice(language: returnLanguage(text))
        synthesizer.speak(utterance)
    }

    override init() {
        super.init()
        synthesizer.delegate = self
    }
    
    // å›žå‚³è¦åˆæˆè²éŸ³çš„èªžè¨€
    func returnLanguage(_ input_text: String) -> String {
        let languageRecognizer = NLLanguageRecognizer()
        languageRecognizer.processString(input_text)
        var Language = ""
        if let dominantLanguage = languageRecognizer.dominantLanguage {
            // åˆ¤æ–·ç‚ºå“ªä¸€ç¨®èªžè¨€
            switch String(dominantLanguage.rawValue) {
            case "en":
                Language = "en-US"
            case "ja":
                Language = "ja-JP"
            case "zh-Hant":
                Language = "zh-TW"
            // æœ‰æ™‚å€™æœƒå¾—åˆ° ç°¡é«”ä¸­æ–‡
            case "zh-Hans":
                Language = "zh-TW"
            default:
                Language = "en-US"
            }
        }
        return Language
    }
    
    
    
}

extension SpeechManager: AVSpeechSynthesizerDelegate {
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didStart utterance: AVSpeechUtterance) {
        DispatchQueue.main.async {
            self.isSpeaking = true
        }
    }

    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didFinish utterance: AVSpeechUtterance) {
        DispatchQueue.main.async {
            self.isSpeaking = false
        }
    }
}
